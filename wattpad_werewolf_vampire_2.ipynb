{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Werewolf and vampire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stories_df = pd.read_csv('data/out/werewolf_vampire_stories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story_id</th>\n",
       "      <th>chapter_id</th>\n",
       "      <th>chapter_index</th>\n",
       "      <th>chapter_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157305</td>\n",
       "      <td>1100896</td>\n",
       "      <td>0</td>\n",
       "      <td>'I packed whilst you were at school' she said ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157305</td>\n",
       "      <td>1108308</td>\n",
       "      <td>1</td>\n",
       "      <td>'Ladies and Gentlemen, this is your captain sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157305</td>\n",
       "      <td>1131027</td>\n",
       "      <td>2</td>\n",
       "      <td>My father was still frowning but he seemed to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157305</td>\n",
       "      <td>1157067</td>\n",
       "      <td>3</td>\n",
       "      <td>Bloody girl, why did she have to turn up...she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157305</td>\n",
       "      <td>1160601</td>\n",
       "      <td>4</td>\n",
       "      <td>Sam x I blushed and looked down... “Umm...well...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   story_id  chapter_id  chapter_index  \\\n",
       "0    157305     1100896              0   \n",
       "1    157305     1108308              1   \n",
       "2    157305     1131027              2   \n",
       "3    157305     1157067              3   \n",
       "4    157305     1160601              4   \n",
       "\n",
       "                                        chapter_text  \n",
       "0  'I packed whilst you were at school' she said ...  \n",
       "1  'Ladies and Gentlemen, this is your captain sp...  \n",
       "2  My father was still frowning but he seemed to ...  \n",
       "3  Bloody girl, why did she have to turn up...she...  \n",
       "4  Sam x I blushed and looked down... “Umm...well...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23978, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = stories_df.chapter_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % (topic_idx + 1))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples = data[:n_samples]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.64 s, sys: 37 µs, total: 2.64 s\n",
      "Wall time: 2.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tf = tf_vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.4 s, sys: 2.95 ms, total: 13.4 s\n",
      "Wall time: 13.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
       "             evaluate_every=-1, learning_decay=0.7,\n",
       "             learning_method='online', learning_offset=50.0,\n",
       "             max_doc_update_iter=100, max_iter=5, mean_change_tol=0.001,\n",
       "             n_jobs=1, n_topics=10, perp_tol=0.1, random_state=0,\n",
       "             topic_word_prior=None, total_samples=1000000.0, verbose=0)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lda.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "emma mike rsquo smiled ya dance room said like eyes know family grabbed looked head felt pulled going ll come\n",
      "\n",
      "Topic #2:\n",
      "sam mom dean just know jacob yeah said dad don hey justin going like oh jake got ok beau look\n",
      "\n",
      "Topic #3:\n",
      "na sa ko la ako que ang el ng kyle ba lo di ya gt oh lt ah love ai\n",
      "\n",
      "Topic #4:\n",
      "harry niall louis liam zayn says just said say like don know boys eyes love ron going room look really\n",
      "\n",
      "Topic #5:\n",
      "agrave ng nh aacute oacute ocirc acirc kh ecirc uacute igrave tr th ch hưu iacute cũng atilde một đi\n",
      "\n",
      "Topic #6:\n",
      "rsquo rdquo ldquo damon elena stefan eyes katherine know head like looked didn just don face going smiled room said\n",
      "\n",
      "Topic #7:\n",
      "gt alex said ian like eyes asked jason girl looked just man face hair lt hand chi little don looking\n",
      "\n",
      "Topic #8:\n",
      "eyes like just didn head felt face away know time looked hand man way did knew door body thought don\n",
      "\n",
      "Topic #9:\n",
      "said looked just asked like eyes walked smiled got head turned room face door know don hair didn look went\n",
      "\n",
      "Topic #10:\n",
      "just like know don didn going said time really want did think say right got ll tell way ve school\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.56 s, sys: 3.8 ms, total: 2.57 s\n",
      "Wall time: 2.56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = NMF(n_components=n_topics, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.95 s, sys: 2.45 s, total: 4.39 s\n",
      "Wall time: 1.75 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NMF(alpha=0.1, beta=1, eta=0.1, init=None, l1_ratio=0.5, max_iter=200,\n",
       "  n_components=10, nls_max_iter=2000, random_state=1, shuffle=False,\n",
       "  solver='cd', sparseness=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "nmf.fit(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "eyes looked like head face felt didn hand away man door room just turned hair way look knew body hands\n",
      "\n",
      "Topic #2:\n",
      "niall liam zayn louis boys pov im guys direction emma yeah phone li date room hey kiss just couch ok\n",
      "\n",
      "Topic #3:\n",
      "said asked walked looked got smiled went started hey nodded laughed mr oh yeah replied turned okay saw door guys\n",
      "\n",
      "Topic #4:\n",
      "just like know don really love didn want time school going think ve ll oh tell people right say day\n",
      "\n",
      "Topic #5:\n",
      "harry louis ron hermione zayn fred boys george uncle love direction room don hand el smiled door eyes hug mum\n",
      "\n",
      "Topic #6:\n",
      "mom dad mother father house parents room school home car phone bed baby family told mum daughter sister going went\n",
      "\n",
      "Topic #7:\n",
      "says say asks ask walk look looks fred eyes door don laugh turn run george head room hear pov grab\n",
      "\n",
      "Topic #8:\n",
      "alex liam hermione mum girl red nodded ron mate lt date world asked school wolf asks ryan year moment parents\n",
      "\n",
      "Topic #9:\n",
      "jacob bella jake vampire mr don aunt car just mrs case beat know dad child said gonna yeah room want\n",
      "\n",
      "Topic #10:\n",
      "dean sam car lt john ron kids uncle pov brother hey trying aunt looking boys quietly man grinned gonna hell\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stories_df.to_csv('data/out/werewolf_vampire_stories.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 0.000s.\n",
      "Extracting tf-idf features for NMF...\n",
      "done in 2.713s.\n",
      "Extracting tf features for LDA...\n",
      "done in 2.644s.\n",
      "Fitting the NMF model with tf-idf features, n_samples=2000 and n_features=1000...\n",
      "done in 1.612s.\n",
      "\n",
      "Topics in NMF model:\n",
      "Topic #0:\n",
      "eyes looked like head face felt didn hand away man door room just turned hair way look knew body hands\n",
      "Topic #1:\n",
      "niall liam zayn louis boys pov im guys direction emma yeah phone li date room hey kiss just couch ok\n",
      "Topic #2:\n",
      "said asked walked looked got smiled went started hey nodded laughed mr oh yeah replied turned okay saw door guys\n",
      "Topic #3:\n",
      "just like know don really love didn want time school going think ve ll oh tell people right say day\n",
      "Topic #4:\n",
      "harry louis ron hermione zayn fred boys george uncle love direction room don hand el smiled door eyes hug mum\n",
      "Topic #5:\n",
      "mom dad mother father house parents room school home car phone bed baby family told mum daughter sister going went\n",
      "Topic #6:\n",
      "says say asks ask walk look looks fred eyes door don laugh turn run george head room hear pov grab\n",
      "Topic #7:\n",
      "alex liam hermione mum girl red nodded ron mate lt date world asked school wolf asks ryan year moment parents\n",
      "Topic #8:\n",
      "jacob bella jake vampire mr don aunt car just mrs case beat know dad child said gonna yeah room want\n",
      "Topic #9:\n",
      "dean sam car lt john ron kids uncle pov brother hey trying aunt looking boys quietly man grinned gonna hell\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 13.466s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0:\n",
      "emma mike rsquo smiled ya dance room said like eyes know family grabbed looked head felt pulled going ll come\n",
      "Topic #1:\n",
      "sam mom dean just know jacob yeah said dad don hey justin going like oh jake got ok beau look\n",
      "Topic #2:\n",
      "na sa ko la ako que ang el ng kyle ba lo di ya gt oh lt ah love ai\n",
      "Topic #3:\n",
      "harry niall louis liam zayn says just said say like don know boys eyes love ron going room look really\n",
      "Topic #4:\n",
      "agrave ng nh aacute oacute ocirc acirc kh ecirc uacute igrave tr th ch hưu iacute cũng atilde một đi\n",
      "Topic #5:\n",
      "rsquo rdquo ldquo damon elena stefan eyes katherine know head like looked didn just don face going smiled room said\n",
      "Topic #6:\n",
      "gt alex said ian like eyes asked jason girl looked just man face hair lt hand chi little don looking\n",
      "Topic #7:\n",
      "eyes like just didn head felt face away know time looked hand man way did knew door body thought don\n",
      "Topic #8:\n",
      "said looked just asked like eyes walked smiled got head turned room face door know don hair didn look went\n",
      "Topic #9:\n",
      "just like know don didn going said time really want did think say right got ll tell way ve school\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Author: Olivier Grisel <olivier.grisel@ensta.org>\n",
    "#         Lars Buitinck\n",
    "#         Chyi-Kwei Yau <chyikwei.yau@gmail.com>\n",
    "# License: BSD 3 clause\n",
    "\n",
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_topics = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic #%d:\" % topic_idx)\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "    print()\n",
    "\n",
    "\n",
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "# dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "#                              remove=('headers', 'footers', 'quotes'))\n",
    "# data_samples = dataset.data[:n_samples]\n",
    "# data_samples = data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf-idf features for NMF.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_topics, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_words)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_topics=n_topics, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['target', 'filenames', 'target_names', 'DESCR', 'data', 'description'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([17,  0, 17, ...,  9,  4,  9])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ '/home/antonio/scikit_learn_data/20news_home/20news-bydate-train/talk.politics.mideast/76141',\n",
       "       '/home/antonio/scikit_learn_data/20news_home/20news-bydate-train/alt.atheism/53281',\n",
       "       '/home/antonio/scikit_learn_data/20news_home/20news-bydate-train/talk.politics.mideast/76350',\n",
       "       ...,\n",
       "       '/home/antonio/scikit_learn_data/20news_home/20news-bydate-train/rec.sport.baseball/105105',\n",
       "       '/home/antonio/scikit_learn_data/20news_home/20news-bydate-train/comp.sys.mac.hardware/51575',\n",
       "       '/home/antonio/scikit_learn_data/20news_home/20news-bydate-train/rec.sport.baseball/104908'], \n",
       "      dtype='<U94')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['filenames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['DESCR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the 20 newsgroups by date dataset'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Well i'm not sure about the story nad it did seem biased. What\\nI disagree with is your statement that the U.S. Media is out to\\nruin Israels reputation. That is rediculous. The U.S. media is\\nthe most pro-israeli media in the world. Having lived in Europe\\nI realize that incidences such as the one described in the\\nletter have occured. The U.S. media as a whole seem to try to\\nignore them. The U.S. is subsidizing Israels existance and the\\nEuropeans are not (at least not to the same degree). So I think\\nthat might be a reason they report more clearly on the\\natrocities.\\n\\tWhat is a shame is that in Austria, daily reports of\\nthe inhuman acts commited by Israeli soldiers and the blessing\\nreceived from the Government makes some of the Holocaust guilt\\ngo away. After all, look how the Jews are treating other races\\nwhen they got power. It is unfortunate.\\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['data'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
