{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Werewolf and vampire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stories_df = pd.read_csv('data/out/werewolf_vampire_stories.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story_id</th>\n",
       "      <th>chapter_id</th>\n",
       "      <th>chapter_index</th>\n",
       "      <th>chapter_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>157305</td>\n",
       "      <td>1100896</td>\n",
       "      <td>0</td>\n",
       "      <td>'I packed whilst you were at school' she said ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>157305</td>\n",
       "      <td>1108308</td>\n",
       "      <td>1</td>\n",
       "      <td>'Ladies and Gentlemen, this is your captain sp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>157305</td>\n",
       "      <td>1131027</td>\n",
       "      <td>2</td>\n",
       "      <td>My father was still frowning but he seemed to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>157305</td>\n",
       "      <td>1157067</td>\n",
       "      <td>3</td>\n",
       "      <td>Bloody girl, why did she have to turn up...she...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>157305</td>\n",
       "      <td>1160601</td>\n",
       "      <td>4</td>\n",
       "      <td>Sam x I blushed and looked down... “Umm...well...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   story_id  chapter_id  chapter_index  \\\n",
       "0    157305     1100896              0   \n",
       "1    157305     1108308              1   \n",
       "2    157305     1131027              2   \n",
       "3    157305     1157067              3   \n",
       "4    157305     1160601              4   \n",
       "\n",
       "                                        chapter_text  \n",
       "0  'I packed whilst you were at school' she said ...  \n",
       "1  'Ladies and Gentlemen, this is your captain sp...  \n",
       "2  My father was still frowning but he seemed to ...  \n",
       "3  Bloody girl, why did she have to turn up...she...  \n",
       "4  Sam x I blushed and looked down... “Umm...well...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23978, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5869"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories_df.story_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = stories_df.chapter_text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23978"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [d for d in data if d is not np.nan]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23972"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "wordnet_word_synsets = {}\n",
    "for d in data:\n",
    "    for w in word_tokenize(d):\n",
    "        if w not in wordnet_word_synsets:\n",
    "            synsets = wn.synsets(w)\n",
    "            if synsets:\n",
    "                wordnet_word_synsets[w] = synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wordnet_word_synsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data = [' '.join([w for w in word_tokenize(d) if w in wordnet_word_synsets]) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = [d for d in data if d.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_topic(topic, feature_names, n_top_words):\n",
    "    return [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "\n",
    "def get_topics(model, feature_names, n_top_words):\n",
    "    topics = []\n",
    "    for topic in model.components_:\n",
    "        topics.append(get_topic(topic, feature_names, n_top_words))\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_top_words(topics):\n",
    "    for topic_idx, topic in enumerate(topics):\n",
    "        print(\"Topic #%d:\" % (topic_idx + 1))\n",
    "        print(\" \".join(topic))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def two_words_similarity(w1, w2):\n",
    "    similarities = [0]\n",
    "    for syns1 in wordnet_word_synsets[w1]:\n",
    "        for syns2 in wordnet_word_synsets[w2]:\n",
    "            similarity = wn.wup_similarity(syns1, syns2) or 0\n",
    "            similarities.append(similarity)\n",
    "    return max(similarities)\n",
    "\n",
    "def word_list_similarity(ws):\n",
    "    similarities = []\n",
    "    for w1 in ws:\n",
    "        for w2 in ws:\n",
    "            if w1 != w2:\n",
    "                similarity = two_words_similarity(w1, w2)\n",
    "                similarities.append(similarity)\n",
    "    return np.mean(similarities)\n",
    "\n",
    "def topic_list_similarity(ts):\n",
    "    similarities = []\n",
    "    for t in ts:\n",
    "        similarity = word_list_similarity(t)\n",
    "        similarities.append(similarity)\n",
    "    return np.mean(similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_FEATURES = 1000\n",
    "N_TOP_WORDS = 20\n",
    "MAX_TOPICS = 50\n",
    "MIN_TOPICS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data_samples = data[:2000]\n",
    "data_samples = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lda_similarities = []\n",
    "for N_TOPICS in range(MIN_TOPICS, MAX_TOPICS + 1):\n",
    "# for N_TOPICS in range(1, 11):\n",
    "    tf_vectorizer = CountVectorizer(\n",
    "        max_df=0.95,\n",
    "        min_df=2,\n",
    "        max_features=N_FEATURES,\n",
    "        stop_words='english',\n",
    "    )\n",
    "    tf = tf_vectorizer.fit_transform(data_samples)\n",
    "    lda = LatentDirichletAllocation(\n",
    "        n_topics=N_TOPICS,\n",
    "        max_iter=5,\n",
    "        learning_method='online',\n",
    "        learning_offset=50.,\n",
    "        random_state=0,\n",
    "    )\n",
    "    lda.fit(tf)\n",
    "    tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "    topics = get_topics(lda, tf_feature_names, N_TOP_WORDS)\n",
    "    similarity = topic_list_similarity(topics)\n",
    "    lda_similarities.append((similarity, topics, len(topics)))\n",
    "lda_winner = max(lda_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(lda_winner[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "nmf_similarities = []\n",
    "for N_TOPICS in range(1, MAX_TOPICS + 1):\n",
    "# for N_TOPICS in range(1, 11):\n",
    "    tfidf_vectorizer = TfidfVectorizer(\n",
    "        max_df=0.95,\n",
    "        min_df=2,\n",
    "        max_features=N_FEATURES,\n",
    "        stop_words='english',\n",
    "    )\n",
    "    tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "    nmf = NMF(\n",
    "        n_components=N_TOPICS,\n",
    "        random_state=1,\n",
    "        alpha=.1,\n",
    "        l1_ratio=.5,\n",
    "    )\n",
    "    nmf.fit(tfidf)\n",
    "    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    topics = get_topics(nmf, tfidf_feature_names, N_TOP_WORDS)\n",
    "    similarity = topic_list_similarity(topics)\n",
    "    nmf_similarities.append((similarity, topics, len(topics)))\n",
    "nmf_winner = max(nmf_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_words(nmf_winner[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stories_df.to_csv('data/out/werewolf_vampire_stories.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
